{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "VideoDataGenerator.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oussamayousre/DLL/blob/version_1/videodatagenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X8CCOVEQZNG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh1AMeGF-9Qi"
      },
      "source": [
        "!git clone https://github.com/metal3d/keras-video-generators.git  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyAeektVCb0Y"
      },
      "source": [
        "!pip install keras-video-generators\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtTJM8lQm5VP",
        "execution": {
          "iopub.status.busy": "2021-08-10T15:27:10.149908Z",
          "iopub.execute_input": "2021-08-10T15:27:10.150225Z",
          "iopub.status.idle": "2021-08-10T15:27:10.170572Z",
          "shell.execute_reply.started": "2021-08-10T15:27:10.150195Z",
          "shell.execute_reply": "2021-08-10T15:27:10.169499Z"
        },
        "trusted": true
      },
      "source": [
        "import math   # for mathematical operations\n",
        "import matplotlib.pyplot as plt    # for plotting the images\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from keras.preprocessing import image   # for preprocessing the images\n",
        "import numpy as np    # for mathematical operations\n",
        "from keras.utils import np_utils\n",
        "from skimage.transform import resize   # for resizing images\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import datasets ,layers,models\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from scipy.spatial import distance as dist\n",
        "#from imutils.video import FileVideoStream\n",
        "from scipy.signal import argrelextrema\n",
        "#from imutils.video import VideoStream\n",
        "#from keras_video import VideoFrameGenerator\n",
        "#from imutils import face_utils\n",
        "import numpy as np\n",
        "import argparse\n",
        "#import imutils\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "# calculate a face embedding for each face in the dataset using facenet\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from keras.models import load_model\n",
        "\n",
        "# calculate a face embedding for each face in the dataset using facenet\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esjgoJZ01Kkx"
      },
      "source": [
        "### \"\"\"arr = os.listdir(\"/content/gdrive/MyDrive/DataBase\")\n",
        "#label_train  = np.array([])\n",
        "#Frame_train = np.array([])\n",
        "i = 0\n",
        "for path_1 in arr :\n",
        "\n",
        "  \n",
        "  \n",
        "  #print(arr)\n",
        "  #verify if the path is a directory or just a hidden file\n",
        "  full_path_1 = f\"/content/gdrive/MyDrive/DataBase/{path_1}\" \n",
        "  isDirectory = os.path.isdir(full_path_1)\n",
        "  if isDirectory : \n",
        "    arr_1 = os.listdir(full_path_1)\n",
        "    \n",
        "    for path_2 in arr_1 : \n",
        "      full_path_2 = f\"/content/gdrive/MyDrive/DataBase/{path_1}/{path_2}\" \n",
        "      arr_2 = os.listdir(full_path_2)\n",
        "      \n",
        "      for path_3 in arr_2 :\n",
        "        i += 1 \n",
        "        label_train  = []\n",
        "        Frame_train = [] \n",
        "        #).reshape(0,1).astype(np.int64)\n",
        "        label = int(os.path.splitext(path_3)[0])\n",
        "        full_path_2 = f\"/content/gdrive/MyDrive/DataBase/{path_1}/{path_2}/{path_3}\"\n",
        "        \n",
        "        vs = cv2.VideoCapture(full_path_2)\n",
        "        while True :\n",
        "          ret, image = vs.read()\n",
        "          if ret == False : \n",
        "            break\n",
        "          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "          image = cv2.resize(image, (224, 224))\n",
        "          #image = image.img_to_array(image)\n",
        "          label_train.append(label)\n",
        "          Frame_train.append(image)\n",
        "        #train classes \n",
        "        np.save(f'/content/gdrive/MyDrive/variables/X_dataset{i}.npy', Frame_train) # save\n",
        "        #train labels\n",
        "        np.save(f'/content/gdrive/MyDrive/variables/y_dataset{i}.npy', label_train) # save\n",
        "        vs.release()\n",
        "        cv2.destroyAllWindows()\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey_zVePI3a_d"
      },
      "source": [
        "#next step is to collect all the videos in one folder and take the first character of their name which is the label \n",
        "import numpy as np\n",
        "import keras\n",
        "#path to video's folder\n",
        "path = '/content/gdrive/MyDrive/variables/Fold2_part2'\n",
        "#\n",
        "train_ID = os.listdir(path)[1:]\n",
        "class VideoFrameGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,train_ID, batch_size=1, dim=(56,56), shuffle=True ):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        self.train_ID = train_ID   \n",
        "        self.shuffle = shuffle\n",
        "        self.dim = dim\n",
        "        self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.train_ID) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.train_ID[k] for k in indexes]\n",
        "        #list_IDs_temp = [20]\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        X =  np.array([X])[0]\n",
        "        y =  np.array([y])\n",
        "        y = y.reshape(y.shape[1],1)\n",
        "        randomize = np.arange(y.shape[0])\n",
        "        np.random.shuffle(randomize)\n",
        "        X = X[randomize]\n",
        "        y = y[randomize]\n",
        "        return X , y \n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.train_ID))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        #X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        #y = np.empty((self.batch_size), dtype=int)\n",
        "        arr_1 = os.listdir(f'/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}')\n",
        "        label_train  = []\n",
        "        Frame_train = [] \n",
        "        # Generate data\n",
        "        for path_3 in arr_1 : \n",
        "          #).reshape(0,1).astype(np.int64)\n",
        "          label = int(os.path.splitext(path_3)[0])\n",
        "          full_path_2 = f\"/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}/{path_3}\"\n",
        "          vs = cv2.VideoCapture(full_path_2)\n",
        "          nb_frame = 0\n",
        "          while True :\n",
        "            ret, image = vs.read()\n",
        "            if ret == False  or nb_frame == 1000  : \n",
        "              break\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = cv2.resize(image, self.dim)\n",
        "            #image = image.img_to_array(image)\n",
        "            label_train.append(label)\n",
        "            Frame_train.append(image)\n",
        "            nb_frame += 1\n",
        "          vs.release()\n",
        "          cv2.destroyAllWindows()\n",
        "        return Frame_train, label_train\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WQjd3_uDlxR"
      },
      "source": [
        "path = '/content/gdrive/MyDrive/variables/Fold2_part2'\n",
        "train_ID = os.listdir(path)[1:]\n",
        "\n",
        "# Parameters\n",
        "params = {'dim': (56,56),\n",
        "          'batch_size': 1,\n",
        "          'shuffle': True}\n",
        "training_generator = VideoFrameGenerator(train_ID, **params)\n",
        "validation_generator = VideoFrameGenerator([19], **params)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AauwousQJoKk"
      },
      "source": [
        "\"\"\"model = tf.keras.models.Sequential([\n",
        "              tf.keras.layers.Conv2D(32, (3,3) , activation = 'relu' ,padding='same', input_shape = (56,56,3), kernel_regularizer=l2(0.01) ),\n",
        "              tf.keras.layers.MaxPool2D(pool_size=(2, 2) ),\n",
        "              #\n",
        "              tf.keras.layers.Conv2D(32 , (3,3) ,padding='same', activation = 'relu',kernel_regularizer=l2(0.01)),\n",
        "              tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
        "              #\n",
        "              tf.keras.layers.Conv2D(64 , (3,3) , activation = 'relu', padding='same', kernel_regularizer=l2(0.01)),\n",
        "              #tf.keras.layers.Dropout(0.2),\n",
        "              tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
        "              #\n",
        "            \n",
        "              #\n",
        "              #  tf.keras.layers.Conv2D(256 , (3,3) , activation = 'relu'),\n",
        "              #  tf.keras.layers.Dropout(0.2),\n",
        "              # tf.keras.layers.BatchNormalization() ,\n",
        "              # tf.keras.layers.MaxPool2D(2,2),\n",
        "              #\n",
        "              tf.keras.layers.Flatten(),\n",
        "              ##\n",
        "              tf.keras.layers.Dense(64 , activation = 'relu', kernel_regularizer=l2(0.01)),\n",
        "              #tf.keras.layers.Dropout(0.2),\n",
        "              tf.keras.layers.Dense(11,activation = 'softmax' ) ]\n",
        "               \n",
        "    )\n",
        "model.summary()\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf0F1fymKKiP"
      },
      "source": [
        "\"\"\"opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer = opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(training_generator, validation_data=validation_generator, epochs = 10 )\n",
        "#change 56 to 224 first thing to do \n",
        "history\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LLvQ_K7g_Wi"
      },
      "source": [
        "x ,y = training_generator.__getitem__(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_eXmU4LjcAg"
      },
      "source": [
        "##############################################################\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7QeXOoFgdu2"
      },
      "source": [
        "!pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRXbkGEKgmYW",
        "execution": {
          "iopub.status.busy": "2021-08-10T15:24:34.538019Z",
          "iopub.execute_input": "2021-08-10T15:24:34.538361Z",
          "iopub.status.idle": "2021-08-10T15:24:52.002990Z",
          "shell.execute_reply.started": "2021-08-10T15:24:34.538324Z",
          "shell.execute_reply": "2021-08-10T15:24:52.002010Z"
        },
        "trusted": true,
        "outputId": "b859085f-4a76-49f2-948c-d13126cea727"
      },
      "source": [
        "! pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "  Downloading gdown-3.13.0.tar.gz (9.3 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: requests[socks]>=2.12.0 in /opt/conda/lib/python3.7/site-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.0.12)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.61.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (1.26.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for gdown: filename=gdown-3.13.0-py3-none-any.whl size=9033 sha256=365aa01f538545e63686acfc5f2ad48c5a2bc49c6a2b525b9121e7de29fd7e38\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/2a/2f/86449b6bdbaa9aef873f68332b68be6bfbc386b9219f47157d\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-3.13.0\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioQMCmWcgjVw"
      },
      "source": [
        "# confirm mtcnn was installed correctly\n",
        "import mtcnn\n",
        "# print version\n",
        "print(mtcnn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfP21302hxp5",
        "execution": {
          "iopub.status.busy": "2021-08-10T15:25:18.089426Z",
          "iopub.execute_input": "2021-08-10T15:25:18.089755Z",
          "iopub.status.idle": "2021-08-10T15:25:22.649360Z",
          "shell.execute_reply.started": "2021-08-10T15:25:18.089725Z",
          "shell.execute_reply": "2021-08-10T15:25:22.648130Z"
        },
        "trusted": true,
        "outputId": "13b3b034-dc10-429d-dd4a-db31e3f8359f"
      },
      "source": [
        "#model 1(facenet on tensorflow v1.x)\n",
        "import gdown\n",
        "!gdown --id 1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1\n",
            "To: /kaggle/working/facenet_keras.h5\n",
            "92.4MB [00:00, 261MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTSOmSXyAZCO"
      },
      "source": [
        "!gdown --id 1W_Kv4vKgKvGf0JU1zSYkvlv_1MpCmg5S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrNBLordieH1"
      },
      "source": [
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "\n",
        "# extract a single face from a given photograph\n",
        "def extract_face(pixels, required_size=(160, 160)): \n",
        "    # create the detector, using default weights   \n",
        "    # detect faces in the image\n",
        "    results = detector.detect_faces(pixels)\n",
        "    if len(results) == 0 : \n",
        "      return np.array([]).reshape(0, 160, 160, 3)\n",
        "    # extract the bounding box from the first face\n",
        "    x1, y1, width, height = results[0]['box']\n",
        "    # bug fix\n",
        "    x1, y1 = abs(x1), abs(y1)\n",
        "    x2, y2 = x1 + width, y1 + height\n",
        "    # extract the face\n",
        "    face = pixels[y1:y2, x1:x2]\n",
        "    # resize pixels to the model size\n",
        "    image = Image.fromarray(face)\n",
        "    image = image.resize(required_size)\n",
        "    face_array = asarray(image)\n",
        "    return face_array\n",
        " \n",
        "# load the photo and extract the face"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNp6tcQ-h0Oz",
        "execution": {
          "iopub.status.busy": "2021-08-10T15:48:42.578522Z",
          "iopub.execute_input": "2021-08-10T15:48:42.578879Z",
          "iopub.status.idle": "2021-08-10T15:48:45.232930Z",
          "shell.execute_reply.started": "2021-08-10T15:48:42.578846Z",
          "shell.execute_reply": "2021-08-10T15:48:45.231237Z"
        },
        "trusted": true,
        "outputId": "2802bf57-1039-4f91-b32d-3b2d59cff4f3"
      },
      "source": [
        "\n",
        "model_facenet = tf.keras.models.load_model('./facenet_keras.h5')\n",
        "print('Loaded Model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQkZhjh7h2kh",
        "execution": {
          "iopub.status.busy": "2021-08-10T15:27:43.097942Z",
          "iopub.execute_input": "2021-08-10T15:27:43.098288Z",
          "iopub.status.idle": "2021-08-10T15:27:43.107254Z",
          "shell.execute_reply.started": "2021-08-10T15:27:43.098257Z",
          "shell.execute_reply": "2021-08-10T15:27:43.106300Z"
        },
        "trusted": true,
        "outputId": "d91c0560-6d9e-4474-ac8b-c0be91285c4a"
      },
      "source": [
        "print(model_facenet.inputs)\n",
        "print(model_facenet.outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n",
            "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI62bCi0iShx"
      },
      "source": [
        "# get the face embedding for one face\n",
        "def get_embedding(model_facenet , face_pixels):\n",
        "    # scale pixel values\n",
        "    face_pixels = face_pixels.astype('float32')\n",
        "    # standardize pixel values across channels (global)\n",
        "    mean, std = face_pixels.mean(), face_pixels.std()\n",
        "    face_pixels = (face_pixels - mean) / std\n",
        "    # transform face into one sample\n",
        "    samples = expand_dims(face_pixels, axis=0)\n",
        "    # make prediction to get embedding\n",
        "    yhat = model_facenet.predict(samples)\n",
        "    return yhat[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwxOfN1SkHDK"
      },
      "source": [
        "### test code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lieRQsP7iWmv"
      },
      "source": [
        "arr_1 = os.listdir('/content/gdrive/MyDrive/variables/Fold3_part1/25')\n",
        "detector = MTCNN()\n",
        "# Generate data\n",
        "for path_3 in arr_1 : \n",
        "  label_train  = np.array([])\n",
        "  Frame_train = np.array([]).reshape(0,128)\n",
        "  #).reshape(0,1).astype(np.int64)\n",
        "  label = int(os.path.splitext(path_3)[0])\n",
        "  full_path_2 = f\"/content/gdrive/MyDrive/variables/Fold3_part1/25/{path_3}\"\n",
        "  #print(label)\n",
        "  vs = cv2.VideoCapture(full_path_2)\n",
        "  nb_frame = 0\n",
        "  #detector = MTCNN()\n",
        "\n",
        "  while True :\n",
        "    \n",
        "    ret, image = vs.read()\n",
        "    if ret == False or nb_frame == 3 : \n",
        "      break\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    #print(image.shape)\n",
        "    image = extract_face(image,required_size=(160, 160))\n",
        "    print(image.shape)\n",
        "    if len(image) == 0  :                \n",
        "        #face_features = np.array([]).reshape(0,128)\n",
        "        nb_frame += 1\n",
        "    else : \n",
        "        face_features = get_embedding(model_facenet , image)             \n",
        "        face_features = np.array(face_features) \n",
        "        face_features = face_features.reshape(1,128)\n",
        "        label_train = np.append(label_train , label)\n",
        "        Frame_train = np.append(Frame_train , face_features ,axis=0)\n",
        "        nb_frame += 1\n",
        "    #image = image.img_to_array(image)\n",
        "    #label_train = np.append(label_train , label)\n",
        "    #Frame_train = np.append(Frame_train , face_features ,axis=0)\n",
        "     \n",
        "  vs.release()\n",
        "  cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVi5BRfTI11H"
      },
      "source": [
        "#next step is to collect all the videos in one folder and take the first character of their name which is the label \n",
        "#path to videos folder\n",
        "path = '/content/gdrive/MyDrive/variables/Fold2_part2'\n",
        "#list of subfolders ID, os.listdir(path)[0] is for the validation data\n",
        "train_ID = os.listdir(path)[1:]\n",
        "class VideoFrameGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,train_ID , model_facenet ,detector = MTCNN(), batch_size=1, dim=(56,56), shuffle=True):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        self.train_ID = train_ID   \n",
        "        self.shuffle = shuffle\n",
        "        self.model_facenet = model_facenet\n",
        "        self.dim = dim\n",
        "        self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.train_ID) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.train_ID[k] for k in indexes]\n",
        "        #list_IDs_temp = [20]\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        #X =  np.array([X])[0]\n",
        "        #y =  np.array([y])\n",
        "        #y = y.reshape(y.shape[1],1)\n",
        "        print(X.shape)\n",
        "        print(y.shape)\n",
        "        return X , y \n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.train_ID))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        #X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        #y = np.empty((self.batch_size), dtype=int)\n",
        "        arr_1 = os.listdir(f'/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}')\n",
        "        # Generate data\n",
        "        label_train  = np.array([]).reshape(0,1)\n",
        "        Frame_train = np.array([]).reshape(0,128)\n",
        "        for path_3 in arr_1 :\n",
        "            #labeling the data using the title of videos \n",
        "            label = int(os.path.splitext(path_3)[0])\n",
        "            full_path_2 = f'/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}/{path_3}'\n",
        "            # Opens the Video file\n",
        "            \n",
        "            vs = cv2.VideoCapture(full_path_2)\n",
        "            nb_frame = 1        \n",
        "            while True :    \n",
        "                ret, image = vs.read()\n",
        "                if ret == False or nb_frame == 3 : \n",
        "                    break\n",
        "                \n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                #print(image.shape)\n",
        "                image = extract_face(image,required_size=(160, 160))\n",
        "                #image = cv2.resize(image, (160,160))\n",
        "                if not image :                \n",
        "                    #face_features = np.array([]).reshape(0,128)\n",
        "                    nb_frame += 1 \n",
        "                else : \n",
        "                    face_features = get_embedding(self.model_facenet , image)             \n",
        "                    face_features = np.array(face_features) \n",
        "                    face_features = face_features.reshape(1,128)\n",
        "                    label_train = np.append(label_train , label)\n",
        "                    Frame_train = np.append(Frame_train , face_features ,axis=0)\n",
        "                    nb_frame += 1 \n",
        "                #image = image.img_to_array(image)\n",
        "                #label_train = np.append(label_train , label)\n",
        "                #Frame_train = np.append(Frame_train , face_features ,axis=0)\n",
        "                \n",
        "            vs.release()\n",
        "            cv2.destroyAllWindows()\n",
        "        return Frame_train, label_train.reshape(label_train.shape[0]).astype(int)\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfkOqeWHRlnj"
      },
      "source": [
        "path = '/content/gdrive/MyDrive/variables/Fold2_part2'\n",
        "train_ID = os.listdir(path)[1:]\n",
        "\n",
        "# Parameters\n",
        "params = {'model_facenet' : model_facenet , \n",
        "          'batch_size': 1,\n",
        "          'shuffle': True}\n",
        "training_generator = VideoFrameGenerator(train_ID, **params)\n",
        "validation_generator = VideoFrameGenerator([os.listdir(path)[0]], **params)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8waLLO04aaf"
      },
      "source": [
        "x ,y = training_generator.__getitem__(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SsB8BTL5OQL"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DaFjMdyenBk"
      },
      "source": [
        "\n",
        "model = models.Sequential([\n",
        "          layers.Dense(256,activation='relu'),\n",
        "          layers.Dense(128,activation='relu'),\n",
        "          layers.Dense(64,activation='relu'),\n",
        "          layers.Dense(32,activation='relu'),\n",
        "          layers.Dense(16,activation='relu'),\n",
        "          layers.Dense(11,activation='softmax'),\n",
        "\n",
        "])\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "history = model.fit(training_generator , validation_data= validation_generator, epochs = 2 )\n",
        "history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akbFgP5AFPs1"
      },
      "source": [
        "## at the part below , I used the Multi-task Cascaded Convolutional Networks (MTCNN) to detect faces in frames and then detect features from those Faces using the FaceNet model , which takes too long comparing to Dlib method(which i'm going to try ) , so the main purpose here is to compare the result(using whole face features) to the one we already found using the eye_aspect_ratio \n",
        "ps : I will not use a sequence model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-10T23:11:20.520316Z",
          "iopub.execute_input": "2021-08-10T23:11:20.520710Z",
          "iopub.status.idle": "2021-08-10T23:11:38.016960Z",
          "shell.execute_reply.started": "2021-08-10T23:11:20.520610Z",
          "shell.execute_reply": "2021-08-10T23:11:38.016034Z"
        },
        "trusted": true,
        "id": "bN1_b0YBN6Cn",
        "outputId": "79441cd5-20b8-4a46-90fc-2b04573841c9"
      },
      "source": [
        "! pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "  Downloading gdown-3.13.0.tar.gz (9.3 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: requests[socks]>=2.12.0 in /opt/conda/lib/python3.7/site-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.0.12)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.61.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (1.26.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for gdown: filename=gdown-3.13.0-py3-none-any.whl size=9033 sha256=1b3d32f016c24132bd43e6bebff5c0b2137cc8f478aa77a45e8d9cd60be90b4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/2a/2f/86449b6bdbaa9aef873f68332b68be6bfbc386b9219f47157d\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-3.13.0\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-10T23:11:38.020279Z",
          "iopub.execute_input": "2021-08-10T23:11:38.020549Z",
          "iopub.status.idle": "2021-08-10T23:11:43.156038Z",
          "shell.execute_reply.started": "2021-08-10T23:11:38.020521Z",
          "shell.execute_reply": "2021-08-10T23:11:43.155173Z"
        },
        "trusted": true,
        "id": "e_dd3NrNN6Cn",
        "outputId": "15118a75-ad8b-4313-989c-8c75ed9beeab"
      },
      "source": [
        "#model 1(facenet on tensorflow v1.x)\n",
        "import gdown\n",
        "!gdown --id 1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1\n",
            "To: /kaggle/working/facenet_keras.h5\n",
            "92.4MB [00:00, 229MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9twyndh4GbwC",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:11:45.724452Z",
          "iopub.execute_input": "2021-08-10T23:11:45.724821Z",
          "iopub.status.idle": "2021-08-10T23:11:51.250893Z",
          "shell.execute_reply.started": "2021-08-10T23:11:45.724786Z",
          "shell.execute_reply": "2021-08-10T23:11:51.250069Z"
        },
        "trusted": true
      },
      "source": [
        "import math   # for mathematical operations\n",
        "import matplotlib.pyplot as plt    # for plotting the images\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from keras.preprocessing import image   # for preprocessing the images\n",
        "import numpy as np    # for mathematical operations\n",
        "from keras.utils import np_utils\n",
        "from skimage.transform import resize   # for resizing images\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "#from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import datasets ,layers,models\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from scipy.spatial import distance as dist\n",
        "#from imutils.video import FileVideoStream\n",
        "from scipy.signal import argrelextrema\n",
        "#from imutils.video import VideoStream\n",
        "#from keras_video import VideoFrameGenerator\n",
        "#from imutils import face_utils\n",
        "import numpy as np\n",
        "import argparse\n",
        "#import imutils\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "# calculate a face embedding for each face in the dataset using facenet\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from keras.models import load_model\n",
        "\n",
        "# calculate a face embedding for each face in the dataset using facenet\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-10T23:12:29.633166Z",
          "iopub.execute_input": "2021-08-10T23:12:29.633484Z",
          "iopub.status.idle": "2021-08-10T23:12:34.367517Z",
          "shell.execute_reply.started": "2021-08-10T23:12:29.633455Z",
          "shell.execute_reply": "2021-08-10T23:12:34.366647Z"
        },
        "trusted": true,
        "id": "6jyeTQvQN6Co",
        "outputId": "3e05408c-1d25-4861-8286-e95eb3333513"
      },
      "source": [
        "\n",
        "model_facenet = tf.keras.models.load_model('./facenet_keras.h5')\n",
        "print('Loaded Model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q8OwcWJaBqW",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:12:34.371530Z",
          "iopub.execute_input": "2021-08-10T23:12:34.373565Z",
          "iopub.status.idle": "2021-08-10T23:12:37.682908Z",
          "shell.execute_reply.started": "2021-08-10T23:12:34.373522Z",
          "shell.execute_reply": "2021-08-10T23:12:37.682033Z"
        },
        "trusted": true,
        "outputId": "3039982f-101f-4e8d-e18a-c84072ea0926"
      },
      "source": [
        "!wget   http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-10 23:12:34--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  26.1MB/s    in 2.3s    \n",
            "\n",
            "2021-08-10 23:12:37 (26.1 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISiAIdJ2caXI",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:12:37.686202Z",
          "iopub.execute_input": "2021-08-10T23:12:37.686478Z",
          "iopub.status.idle": "2021-08-10T23:12:44.733554Z",
          "shell.execute_reply.started": "2021-08-10T23:12:37.686449Z",
          "shell.execute_reply": "2021-08-10T23:12:44.732481Z"
        },
        "trusted": true
      },
      "source": [
        "!bunzip2 ./shape_predictor_68_face_landmarks.dat.bz2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al_Q6bwbi3Sc",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:12:44.737359Z",
          "iopub.execute_input": "2021-08-10T23:12:44.737630Z",
          "iopub.status.idle": "2021-08-10T23:12:46.154485Z",
          "shell.execute_reply.started": "2021-08-10T23:12:44.737601Z",
          "shell.execute_reply": "2021-08-10T23:12:46.153635Z"
        },
        "trusted": true,
        "outputId": "34edae09-1219-4a81-bf31-894c578bd1e2"
      },
      "source": [
        "!wget https://github.com/davisking/dlib-models/raw/master/mmod_human_face_detector.dat.bz2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-10 23:12:45--  https://github.com/davisking/dlib-models/raw/master/mmod_human_face_detector.dat.bz2\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/davisking/dlib-models/master/mmod_human_face_detector.dat.bz2 [following]\n",
            "--2021-08-10 23:12:45--  https://raw.githubusercontent.com/davisking/dlib-models/master/mmod_human_face_detector.dat.bz2\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 694687 (678K) [application/octet-stream]\n",
            "Saving to: ‘mmod_human_face_detector.dat.bz2’\n",
            "\n",
            "mmod_human_face_det 100%[===================>] 678.41K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-08-10 23:12:46 (21.0 MB/s) - ‘mmod_human_face_detector.dat.bz2’ saved [694687/694687]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ-NYfl5jFvB",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:12:46.156696Z",
          "iopub.execute_input": "2021-08-10T23:12:46.157072Z",
          "iopub.status.idle": "2021-08-10T23:12:46.874133Z",
          "shell.execute_reply.started": "2021-08-10T23:12:46.157026Z",
          "shell.execute_reply": "2021-08-10T23:12:46.873065Z"
        },
        "trusted": true
      },
      "source": [
        "!bunzip2 ./mmod_human_face_detector.dat.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPLYgv-Ccd2Z",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:12:46.877469Z",
          "iopub.execute_input": "2021-08-10T23:12:46.877761Z",
          "iopub.status.idle": "2021-08-10T23:12:47.362289Z",
          "shell.execute_reply.started": "2021-08-10T23:12:46.877731Z",
          "shell.execute_reply": "2021-08-10T23:12:47.361335Z"
        },
        "trusted": true
      },
      "source": [
        "detector = dlib.get_frontal_face_detector()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJfgjmOZBuXM",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:12:47.363845Z",
          "iopub.execute_input": "2021-08-10T23:12:47.364379Z",
          "iopub.status.idle": "2021-08-10T23:12:47.373534Z",
          "shell.execute_reply.started": "2021-08-10T23:12:47.364340Z",
          "shell.execute_reply": "2021-08-10T23:12:47.372514Z"
        },
        "trusted": true
      },
      "source": [
        "# get the face embedding for one face\n",
        "def get_embedding(model_facenet , face_pixels):\n",
        "    # scale pixel values\n",
        "    face_pixels = face_pixels.astype('float32')\n",
        "    # standardize pixel values across channels (global)\n",
        "    mean, std = face_pixels.mean(), face_pixels.std()\n",
        "    face_pixels = (face_pixels - mean) / std\n",
        "    # transform face into one sample\n",
        "    samples = expand_dims(face_pixels, axis=0)\n",
        "    # make prediction to get embedding\n",
        "    yhat = model_facenet.predict(samples)\n",
        "    return yhat[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSUAUwckcju0",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:38:29.616992Z",
          "iopub.execute_input": "2021-08-10T23:38:29.617535Z",
          "iopub.status.idle": "2021-08-10T23:38:29.655974Z",
          "shell.execute_reply.started": "2021-08-10T23:38:29.617485Z",
          "shell.execute_reply": "2021-08-10T23:38:29.654749Z"
        },
        "trusted": true
      },
      "source": [
        "#next step is to collect all the videos in one folder and take the first character of their name which is the label \n",
        "#path to videos folder\n",
        "#path = '/content/gdrive/MyDrive/variables/Fold2_part2'\n",
        "#list of subfolders ID, os.listdir(path)[0] is for the validation data\n",
        "#train_ID = os.listdir(path)[1:]\n",
        "class VideoFrameGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,train_ID ,model_facenet,  batch_size=1, dim=(56,56), shuffle=True):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        self.train_ID = train_ID   \n",
        "        self.shuffle = shuffle\n",
        "        self.model_facenet = model_facenet\n",
        "        self.dim = dim\n",
        "        self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.train_ID) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.train_ID[k] for k in indexes]\n",
        "        #list_IDs_temp = [20]\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        #X =  np.array([X])[0]\n",
        "        #y =  np.array([y])\n",
        "        #y = y.reshape(y.shape[1],1)\n",
        "        return X , y \n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.train_ID))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        #X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        #y = np.empty((self.batch_size), dtype=int)\n",
        "        arr_1 = os.listdir(f'../input/fold2-part2/Fold2_part2/Fold2_part2/{list_IDs_temp[0]}')\n",
        "        # Generate data\n",
        "        label_train  = np.array([]).reshape(0,1)\n",
        "        Frame_train = np.array([]).reshape(0,128)\n",
        "        for path_3 in arr_1 :  \n",
        "            #labeling the data using the title of videos \n",
        "            label = int(os.path.splitext(path_3)[0])\n",
        "            full_path_2 = f'../input/fold2-part2/Fold2_part2/Fold2_part2/{list_IDs_temp[0]}/{path_3}'\n",
        "            # Opens the Video file\n",
        "            vs = cv2.VideoCapture(full_path_2)\n",
        "            nb_frame = 1  \n",
        "            while True : \n",
        "                 \n",
        "                ret, image = vs.read()\n",
        "                if ret == False  or nb_frame ==  5000 : \n",
        "                    break      \n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                  #print(image.shape)\n",
        "                rects = detector(image, 0)\n",
        "                for face in rects:\n",
        "                    x1 = face.left() # left point\n",
        "                    y1 = face.top() # top point\n",
        "                    x2 = face.right() # right point\n",
        "                    y2 = face.bottom() # bottom point\n",
        "                    img = image[y1:y2,x1:x2]\n",
        "                    img = cv2.resize(image, (160,160))\n",
        "                    \n",
        "                    #if  img.size == 0 :\n",
        "                        #print(nb_frame)\n",
        "                        #face_features = np.array([]).reshape(0,128)\n",
        "                       # nb_frame += 1 \n",
        "                    if img.size != 0 :          \n",
        "                        face_features = get_embedding(self.model_facenet , img)                \n",
        "                        face_features = np.array(face_features) \n",
        "                        face_features = face_features.reshape(1,128)\n",
        "                        label_train = np.append(label_train , label)\n",
        "                        Frame_train = np.append(Frame_train , face_features ,axis=0)\n",
        "                        nb_frame += 1 \n",
        "                #image = image.img_to_array(image)\n",
        "                #label_train = np.append(label_train , label)\n",
        "                #Frame_train = np.append(Frame_train , face_features ,axis=0)\n",
        "              \n",
        "            #vs.release()\n",
        "            #cv2.destroyAllWindows()\n",
        "        return Frame_train, label_train.reshape(label_train.shape[0]).astype(int)\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQ1pE6pYFUw",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:38:32.031356Z",
          "iopub.execute_input": "2021-08-10T23:38:32.031672Z",
          "iopub.status.idle": "2021-08-10T23:38:32.041900Z",
          "shell.execute_reply.started": "2021-08-10T23:38:32.031640Z",
          "shell.execute_reply": "2021-08-10T23:38:32.040948Z"
        },
        "trusted": true
      },
      "source": [
        "path = '../input/fold2-part2/Fold2_part2/Fold2_part2'\n",
        "arr_Id = os.listdir(path)\n",
        "train_ID = arr_Id[1:]\n",
        "# Parameters\n",
        "params = {'model_facenet' : model_facenet , \n",
        "          'batch_size': 1,\n",
        "          'shuffle': True}\n",
        "training_generator = VideoFrameGenerator(train_ID, **params)\n",
        "validation_generator = VideoFrameGenerator([arr_Id[0]], **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR0kBvOQYgdB",
        "trusted": true
      },
      "source": [
        "x ,y = training_generator.__getitem__(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bJoNLMzebyf",
        "execution": {
          "iopub.status.busy": "2021-08-10T23:38:38.671114Z",
          "iopub.execute_input": "2021-08-10T23:38:38.671444Z"
        },
        "trusted": true,
        "outputId": "f4e10347-eb34-48af-e991-6d6f8174ebf0"
      },
      "source": [
        "model = models.Sequential([\n",
        "          layers.Dense(256,activation='relu'),\n",
        "          layers.Dense(128,activation='relu'),\n",
        "          layers.Dense(64,activation='relu'),\n",
        "          layers.Dense(32,activation='relu'),\n",
        "          layers.Dense(16,activation='relu'),\n",
        "          layers.Dense(11,activation='softmax'),\n",
        "\n",
        "])\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "checkpoint_path = \"./cp.ckpt\"\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_best_only=True, save_weights_only=True, verbose=1,save_freq=1)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "history = model.fit(training_generator , validation_data= validation_generator, epochs = 10, callbacks=[cp_callback] )\n",
        "history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/5 [=====>........................] - ETA: 4:38:01 - loss: 2.3133 - accuracy: 0.3330"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ueom2ODektq"
      },
      "source": [
        "#load pre-trained weights\n",
        "model.load_weights(checkpoint_path)\n",
        "history = model.fit(training_generator , validation_data= validation_generator, epochs = 10, callbacks=[cp_callback] )\n",
        "history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LwhvXeseqRC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
