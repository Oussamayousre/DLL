{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video_Generator_VGG16.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oussamayousre/DLL/blob/version_1/Video_Generator_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCkVdSIFsBzZ",
        "outputId": "908c9c2c-5864-446b-aeba-fdea2f6e8fea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAQHJsmhtgbe"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets ,layers,models\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.applications import *\n",
        "import dlib\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import os \n",
        "import cv2\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.utils import Sequence\n",
        "# i used a keras model , so i have to use data_utils.Sequence\n",
        "from keras.utils import data_utils\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtaqNSlij8hg"
      },
      "source": [
        "detector = dlib.get_frontal_face_detector()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAuQij6WqaHX",
        "outputId": "02f27ffb-57ad-492e-9a63-ee68b8cbedf7"
      },
      "source": [
        "!wget http://dlib.net/files/mmod_human_face_detector.dat.bz2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-16 21:03:00--  http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 694709 (678K)\n",
            "Saving to: ‘mmod_human_face_detector.dat.bz2.1’\n",
            "\n",
            "mmod_human_face_det 100%[===================>] 678.43K  1.60MB/s    in 0.4s    \n",
            "\n",
            "2021-08-16 21:03:00 (1.60 MB/s) - ‘mmod_human_face_detector.dat.bz2.1’ saved [694709/694709]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB8FeubVqcKo",
        "outputId": "cee91cac-79de-4081-8918-d4e7494b7ce6"
      },
      "source": [
        "!bunzip2 /content/mmod_human_face_detector.dat.bz2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bunzip2: Output file /content/mmod_human_face_detector.dat already exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyw2rfHTrJwL"
      },
      "source": [
        "cnn_face_detector = dlib.cnn_face_detection_model_v1('/content/mmod_human_face_detector.dat')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_6Iy1HDsFDa"
      },
      "source": [
        "#next step is to collect all the videos in one folder and take the first character of their name which is the label \n",
        "import numpy as np\n",
        "import keras\n",
        "#path to video's folder\n",
        "path = '/content/gdrive/MyDrive/variables/Fold2_part2'\n",
        "#\n",
        "train_ID = os.listdir(path)[1:]\n",
        "class VideoFrameGenerator(data_utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self,train_ID, batch_size=1, dim=(64,64), shuffle=True ):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        self.train_ID = train_ID   \n",
        "        self.shuffle = shuffle\n",
        "        self.dim = dim\n",
        "        self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.train_ID) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.train_ID[k] for k in indexes]\n",
        "        #list_IDs_temp = [20]\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        X =  np.array([X])[0]\n",
        "        y =  np.array([y])\n",
        "        y = y.reshape(y.shape[1])\n",
        "        randomize = np.arange(y.shape[0])\n",
        "        np.random.shuffle(randomize)\n",
        "        X = X[randomize]\n",
        "        y = y[randomize]\n",
        "        return X , y \n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.train_ID))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        #X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        #y = np.empty((self.batch_size), dtype=int)\n",
        "        arr_1 = os.listdir(f'/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}')\n",
        "        path_1  = f'/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}'\n",
        "        isDirectory = os.path.isdir(path_1)\n",
        "        if isDirectory : \n",
        "          label_train  = []\n",
        "          Frame_train = [] \n",
        "          # Generate data\n",
        "          for path_3 in arr_1 :  \n",
        "            if path_3 != '.ipynb_checkpoints' : \n",
        "                       \n",
        "              label = int(os.path.splitext(path_3)[0])\n",
        "              full_path_2 = f\"/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}/{path_3}\"\n",
        "              vs = cv2.VideoCapture(full_path_2)\n",
        "              nb_frame = 0\n",
        "              while True :\n",
        "                ret, image = vs.read() \n",
        "                if ret == False  or nb_frame == 5000 : \n",
        "                    break\n",
        "                #rects = detector(image,0)\n",
        "                rects = detector(image, 0)\n",
        "                for face in rects:                          \n",
        "                  x1 = face.left() # left point\n",
        "                  y1 = face.top() # top point\n",
        "                  x2 = face.right() # right point\n",
        "                  y2 = face.bottom() # bottom point\n",
        "                  img = image[y1:y2,x1:x2]              \n",
        "                  if img.size != 0 :\n",
        "                    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    img = cv2.resize(image, self.dim)\n",
        "                    #image = image.img_to_array(image)\n",
        "                    label_train.append(label)\n",
        "                    Frame_train.append(img)\n",
        "                    nb_frame += 1\n",
        "              vs.release()\n",
        "              cv2.destroyAllWindows()\n",
        "        \n",
        "        return Frame_train, label_train\n",
        "     "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjNoXriOs7hy"
      },
      "source": [
        "path = '/content/gdrive/MyDrive/variables/Fold2_part2'\n",
        "train_ID = os.listdir(path)[1:]\n",
        "\n",
        "# Parameters\n",
        "params = {'dim': (64,64),\n",
        "          'batch_size': 1,\n",
        "          'shuffle': True}\n",
        "training_generator = VideoFrameGenerator(train_ID, **params)\n",
        "validation_generator = VideoFrameGenerator([train_ID[0]], **params)\n",
        " "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63TCHBtYx-ib"
      },
      "source": [
        "x ,y = training_generator.__getitem__(0)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9sv70WnyIH_",
        "outputId": "f641ea42-6985-41d5-9185-44270f5b548f"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 64, 64, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFJ6wrnWtVzs"
      },
      "source": [
        "vgg16 = VGG16( include_top=False, input_shape = (64,64, 3) , weights = 'imagenet' )\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7QD9TxmuCY4"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "for layer in vgg16.layers[:-1]: # this is where I changed your code\n",
        "    model.add(layer)    \n",
        "# Freeze the layers \n",
        "#for layer in model.layers:\n",
        "#    layer.trainable = False"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA1e7TyTvdZc"
      },
      "source": [
        "model.add(Flatten())\n",
        "model.add(Dense(11, activation='softmax'))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BglYwwmCvj7i",
        "outputId": "7720cccb-46cd-495c-f04a-ff4c8b572edf"
      },
      "source": [
        "model.build((0,64,64,3))\n",
        "model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 11)                90123     \n",
            "=================================================================\n",
            "Total params: 14,804,811\n",
            "Trainable params: 14,804,811\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOrzHjfHvlM4"
      },
      "source": [
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "model.compile(optimizer = opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L197gOUavvvD"
      },
      "source": [
        "# try to capture face then train the model on faces directly\n",
        "history = model.fit(training_generator, validation_data=validation_generator, epochs = 10 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMkwpt15LJlz"
      },
      "source": [
        "model.save('/content/gdrive/MyDrive/variables')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu_ArHO12EZv"
      },
      "source": [
        "vs = cv2.VideoCapture(\"/content/gdrive/MyDrive/variables/dataset/0/0.mp4\")\n",
        "Frame_train = np.array([]).reshape(0,128)\n",
        "label_train  = np.array([]).reshape(0,1)\n",
        "COUNTER = 0 \n",
        "while True : \n",
        "    ret, image = vs.read()\n",
        "    if ret == False  : \n",
        "        break      \n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(image, (64,64))\n",
        "    img = img.reshape(1,64,64,3)\n",
        "    pred = np.argmax(model.predict(img))\n",
        "    print(pred)\n",
        "  #  cv2_imshow(frame)\n",
        " #   key = cv2.waitKey(1) & 0xFF\n",
        "  #  if key == ord(\"q\"):\n",
        "       #     break\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c_cB3PyiZXU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}